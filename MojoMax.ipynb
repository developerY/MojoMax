{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPqCYRC8lJjEr6eEwFsKnZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b449b81f0b44802b3a882f6f6723eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5180f0ceb6d48d78f3f5da39565085f",
              "IPY_MODEL_2ec7ee85fef649d9b730dd98052e3712",
              "IPY_MODEL_73731502254149b783b4b63017213d82"
            ],
            "layout": "IPY_MODEL_c893802bc6ad4b59a3475579dae8f735"
          }
        },
        "a5180f0ceb6d48d78f3f5da39565085f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b527ec888bd41f2b59b811944b7306d",
            "placeholder": "​",
            "style": "IPY_MODEL_454d18de66b04676a9c76c590ad5a23e",
            "value": "config.json: 100%"
          }
        },
        "2ec7ee85fef649d9b730dd98052e3712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bac9734553a4f33a7085c9875fd2be8",
            "max": 659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f37e1161fc04694bc6e2c1cdc07baa2",
            "value": 659
          }
        },
        "73731502254149b783b4b63017213d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d4aa292258f40339823b949d76e0825",
            "placeholder": "​",
            "style": "IPY_MODEL_3c8c5a1fccc6408b8336bac4c4fe3ceb",
            "value": " 659/659 [00:00&lt;00:00, 64.0kB/s]"
          }
        },
        "c893802bc6ad4b59a3475579dae8f735": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b527ec888bd41f2b59b811944b7306d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "454d18de66b04676a9c76c590ad5a23e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bac9734553a4f33a7085c9875fd2be8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f37e1161fc04694bc6e2c1cdc07baa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d4aa292258f40339823b949d76e0825": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c8c5a1fccc6408b8336bac4c4fe3ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/developerY/MojoMax/blob/main/MojoMax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "INR5nxgRj3bA",
        "outputId": "f3e619a2-d76e-463e-863c-586c4f3c88e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://dl.modular.com/public/nightly/python/simple/, https://download.pytorch.org/whl/cpu\n",
            "Collecting modular\n",
            "  Downloading https://dl.modular.com/public/nightly/python/modular-25.5.0-py3-none-any.whl (1.4 kB)\n",
            "Collecting max==25.5.0 (from max[serve]==25.5.0->modular)\n",
            "  Downloading https://dl.modular.com/public/nightly/python/max-25.5.0-py3-none-manylinux_2_34_x86_64.whl (302.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mblack==25.5.0 (from max==25.5.0->max[serve]==25.5.0->modular)\n",
            "  Downloading https://dl.modular.com/public/nightly/python/mblack-25.5.0-py3-none-any.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.2/146.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.12/dist-packages (from max==25.5.0->max[serve]==25.5.0->modular) (2.0.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (8.2.1)\n",
            "Collecting gguf>=0.17.1 (from max[serve]==25.5.0->modular)\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: hf-transfer>=0.1.9 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (0.1.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (0.34.4)\n",
            "Requirement already satisfied: jinja2>=3.1.6 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (3.1.6)\n",
            "Collecting llguidance>=1.0.1 (from max[serve]==25.5.0->modular)\n",
            "  Downloading llguidance-1.2.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (11.3.0)\n",
            "Collecting psutil>=6.1.1 (from max[serve]==25.5.0->modular)\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.52.4 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (4.55.4)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (0.35.0)\n",
            "Collecting uvloop>=0.21.0 (from max[serve]==25.5.0->modular)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (24.1.0)\n",
            "Collecting asgiref>=3.8.1 (from max[serve]==25.5.0->modular)\n",
            "  Downloading asgiref-3.9.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting fastapi<0.116,>=0.115.3 (from max[serve]==25.5.0->modular)\n",
            "  Downloading fastapi-0.115.14-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: grpcio>=1.68.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (1.74.0)\n",
            "Requirement already satisfied: httpx<0.29,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (0.28.1)\n",
            "Collecting msgspec>=0.19.0 (from max[serve]==25.5.0->modular)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.29.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (1.36.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http>=1.27.0 (from max[serve]==25.5.0->modular)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-prometheus>=0.50b0 (from max[serve]==25.5.0->modular)\n",
            "  Downloading opentelemetry_exporter_prometheus-0.57b0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-sdk<1.36.0,>=1.29.0 (from max[serve]==25.5.0->modular)\n",
            "  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (0.22.1)\n",
            "Collecting protobuf<6.32.0,>=6.31.1 (from max[serve]==25.5.0->modular)\n",
            "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: pydantic-settings>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (2.10.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (2.11.7)\n",
            "Collecting pyinstrument>=5.0.1 (from max[serve]==25.5.0->modular)\n",
            "  Downloading pyinstrument-5.1.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (26 kB)\n",
            "Collecting python-json-logger>=2.0.7 (from max[serve]==25.5.0->modular)\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting pyzmq>=26.3.0 (from max[serve]==25.5.0->modular)\n",
            "  Downloading pyzmq-27.0.2-cp312-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: regex>=2024.11.6 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (2024.11.6)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (1.16.1)\n",
            "Requirement already satisfied: sse-starlette>=2.1.2 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (3.0.2)\n",
            "Collecting starlette<0.41.3,>=0.40.0 (from max[serve]==25.5.0->modular)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting taskgroup>=0.2.2 (from max[serve]==25.5.0->modular)\n",
            "  Downloading taskgroup-0.2.2-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.5.0->modular) (0.21.4)\n",
            "Collecting mypy-extensions>=0.4.3 (from mblack==25.5.0->max==25.5.0->max[serve]==25.5.0->modular)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from mblack==25.5.0->max==25.5.0->max[serve]==25.5.0->modular)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from mblack==25.5.0->max==25.5.0->max[serve]==25.5.0->modular) (4.3.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from gguf>=0.17.1->max[serve]==25.5.0->modular) (6.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.28.1->max[serve]==25.5.0->modular) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.28.1->max[serve]==25.5.0->modular) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.28.1->max[serve]==25.5.0->modular) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.28.1->max[serve]==25.5.0->modular) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29,>=0.28.1->max[serve]==25.5.0->modular) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.0->max[serve]==25.5.0->modular) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.0->max[serve]==25.5.0->modular) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.0->max[serve]==25.5.0->modular) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.0->max[serve]==25.5.0->modular) (1.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1.6->max[serve]==25.5.0->modular) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.29.0->max[serve]==25.5.0->modular) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.27.0->max[serve]==25.5.0->modular) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-http>=1.27.0->max[serve]==25.5.0->modular)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-http>=1.27.0->max[serve]==25.5.0->modular)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-exporter-otlp-proto-http to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-otlp-proto-http>=1.27.0 (from max[serve]==25.5.0->modular)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0 (from opentelemetry-exporter-otlp-proto-http>=1.27.0->max[serve]==25.5.0->modular)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.35.0 (from opentelemetry-exporter-otlp-proto-http>=1.27.0->max[serve]==25.5.0->modular)\n",
            "  Downloading opentelemetry_proto-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-exporter-prometheus to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-prometheus>=0.50b0 (from max[serve]==25.5.0->modular)\n",
            "  Downloading opentelemetry_exporter_prometheus-0.56b0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-api>=1.29.0 (from max[serve]==25.5.0->modular)\n",
            "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk<1.36.0,>=1.29.0->max[serve]==25.5.0->modular)\n",
            "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->max[serve]==25.5.0->modular) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->max[serve]==25.5.0->modular) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->max[serve]==25.5.0->modular) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.7.1->max[serve]==25.5.0->modular) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->max[serve]==25.5.0->modular) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->max[serve]==25.5.0->modular) (2.5.0)\n",
            "Collecting exceptiongroup (from taskgroup>=0.2.2->max[serve]==25.5.0->modular)\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.52.4->max[serve]==25.5.0->modular) (0.6.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29,>=0.28.1->max[serve]==25.5.0->modular) (1.3.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.29.0->max[serve]==25.5.0->modular) (3.23.0)\n",
            "Downloading asgiref-3.9.1-py3-none-any.whl (23 kB)\n",
            "Downloading fastapi-0.115.14-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-1.2.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.35.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_prometheus-0.56b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyinstrument-5.1.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (146 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.8/146.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading pyzmq-27.0.2-cp312-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (840 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.6/840.6 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading taskgroup-0.2.2-py2.py3-none-any.whl (14 kB)\n",
            "Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: uvloop, pyzmq, python-json-logger, pyinstrument, psutil, protobuf, pathspec, mypy-extensions, msgspec, llguidance, gguf, exceptiongroup, asgiref, taskgroup, starlette, opentelemetry-proto, opentelemetry-api, mblack, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, max, fastapi, opentelemetry-sdk, opentelemetry-exporter-prometheus, opentelemetry-exporter-otlp-proto-http, modular\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 26.2.1\n",
            "    Uninstalling pyzmq-26.2.1:\n",
            "      Successfully uninstalled pyzmq-26.2.1\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.47.3\n",
            "    Uninstalling starlette-0.47.3:\n",
            "      Successfully uninstalled starlette-0.47.3\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.36.0\n",
            "    Uninstalling opentelemetry-api-1.36.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.36.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.57b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.57b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.57b0\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.116.1\n",
            "    Uninstalling fastapi-0.116.1:\n",
            "      Successfully uninstalled fastapi-0.116.1\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.36.0\n",
            "    Uninstalling opentelemetry-sdk-1.36.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.36.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\n",
            "google-adk 1.12.0 requires starlette<1.0.0,>=0.46.2, but you have starlette 0.41.2 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.31.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.9.1 exceptiongroup-1.3.0 fastapi-0.115.14 gguf-0.17.1 llguidance-1.2.0 max-25.5.0 mblack-25.5.0 modular-25.5.0 msgspec-0.19.0 mypy-extensions-1.1.0 opentelemetry-api-1.35.0 opentelemetry-exporter-otlp-proto-common-1.35.0 opentelemetry-exporter-otlp-proto-http-1.35.0 opentelemetry-exporter-prometheus-0.56b0 opentelemetry-proto-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 pathspec-0.12.1 protobuf-6.31.1 psutil-7.0.0 pyinstrument-5.1.1 python-json-logger-3.3.0 pyzmq-27.0.2 starlette-0.41.2 taskgroup-0.2.2 uvloop-0.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "psutil",
                  "zmq"
                ]
              },
              "id": "e5bc1e0217874ce99d974a7eadc436a4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install modular --index-url https://dl.modular.com/public/nightly/python/simple/ --extra-index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KLTegwJqsUJ",
        "outputId": "7804cae9-7351-490b-9fd9-3791589c7fe6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Aug 28 03:14:01 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from max.driver import CPU, Accelerator, Tensor, accelerator_count\n",
        "from max.dtype import DType\n",
        "from max.engine import InferenceSession\n",
        "from max.graph import DeviceRef, Graph, TensorType, ops"
      ],
      "metadata": {
        "id": "0jE1oNg7rACu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accelerator_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pltftEJrKIJ",
        "outputId": "e5f9759f-9a65-455d-c672-e38340ae08fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = CPU() if accelerator_count() == 0 else Accelerator()\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZck1H90rPwm",
        "outputId": "8a6845ec-6785-4687-e5fa-6f530b31912f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Device(type=gpu,id=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_width = 10\n",
        "dtype = DType.float32\n",
        "\n",
        "with Graph(\n",
        "    \"vector_addition\",\n",
        "    input_types=[\n",
        "        TensorType(\n",
        "            dtype,\n",
        "            shape=[vector_width],\n",
        "            device=DeviceRef.from_device(device),\n",
        "        ),\n",
        "        TensorType(\n",
        "            dtype,\n",
        "            shape=[vector_width],\n",
        "            device=DeviceRef.from_device(device),\n",
        "        ),\n",
        "    ],\n",
        ") as graph:\n",
        "    lhs, rhs = graph.inputs\n",
        "    output = lhs + rhs\n",
        "    graph.output(output)"
      ],
      "metadata": {
        "id": "puZgX5XSraYN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = InferenceSession(\n",
        "    devices=[device],\n",
        ")\n",
        "\n",
        "model = session.load(graph)"
      ],
      "metadata": {
        "id": "q1BTtl7KrgYi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lhs_values = np.random.uniform(size=(vector_width)).astype(np.float32)\n",
        "rhs_values = np.random.uniform(size=(vector_width)).astype(np.float32)\n",
        "\n",
        "lhs_tensor = Tensor.from_numpy(lhs_values).to(device)\n",
        "rhs_tensor = Tensor.from_numpy(rhs_values).to(device)"
      ],
      "metadata": {
        "id": "xRMLB4ifbPjx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.execute(lhs_tensor, rhs_tensor)[0]\n",
        "\n",
        "result = result.to(CPU())"
      ],
      "metadata": {
        "id": "D8DXSehybTFm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_QD15XwSr1Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Left-hand-side values:\")\n",
        "print(lhs_values)\n",
        "print()\n",
        "\n",
        "print(\"Right-hand-side values:\")\n",
        "print(rhs_values)\n",
        "print()\n",
        "\n",
        "print(\"Graph result:\")\n",
        "print(result.to_numpy())\n",
        "print()\n",
        "\n",
        "print(\"Expected result:\")\n",
        "print(lhs_values + rhs_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrgRlySWbVLZ",
        "outputId": "7f786213-1fd8-4d60-b821-808f5a93b7b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Left-hand-side values:\n",
            "[0.78326505 0.80520815 0.7258589  0.30281925 0.35992864 0.77140385\n",
            " 0.43595538 0.46135175 0.95262194 0.08872832]\n",
            "\n",
            "Right-hand-side values:\n",
            "[0.17332724 0.33674908 0.1111757  0.11207277 0.36297217 0.8505733\n",
            " 0.30538553 0.22039181 0.6334334  0.1559453 ]\n",
            "\n",
            "Graph result:\n",
            "[0.9565923  1.1419573  0.83703464 0.41489202 0.7229008  1.6219771\n",
            " 0.7413409  0.68174356 1.5860553  0.24467361]\n",
            "\n",
            "Expected result:\n",
            "[0.9565923  1.1419573  0.83703464 0.41489202 0.7229008  1.6219771\n",
            " 0.7413409  0.68174356 1.5860553  0.24467361]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from max.entrypoints.llm import LLM\n",
        "from max.pipelines import PipelineConfig\n",
        "from max.serve.config import Settings"
      ],
      "metadata": {
        "id": "6xKaR7qaT_zE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "e9ab7254-40c3-41bc-8e2b-2e160b61b2ba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "print(f\"Loading model: {model_path}\")\n",
        "pipeline_config = PipelineConfig(model_path=model_path)\n",
        "settings = Settings()\n",
        "llm = LLM(settings, pipeline_config)\n",
        "\n",
        "prompts = [\n",
        "    \"The fastest way to learn python is\",\n",
        "]\n",
        "\n",
        "print(\"Generating responses...\")\n",
        "responses = llm.generate(prompts, max_new_tokens=50)\n",
        "\n",
        "for i, (prompt, response) in enumerate(zip(prompts, responses)):\n",
        "    print(f\"========== Response {i} ==========\")\n",
        "    print(prompt + response)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488,
          "referenced_widgets": [
            "9b449b81f0b44802b3a882f6f6723eb1",
            "a5180f0ceb6d48d78f3f5da39565085f",
            "2ec7ee85fef649d9b730dd98052e3712",
            "73731502254149b783b4b63017213d82",
            "c893802bc6ad4b59a3475579dae8f735",
            "0b527ec888bd41f2b59b811944b7306d",
            "454d18de66b04676a9c76c590ad5a23e",
            "7bac9734553a4f33a7085c9875fd2be8",
            "3f37e1161fc04694bc6e2c1cdc07baa2",
            "9d4aa292258f40339823b949d76e0825",
            "3c8c5a1fccc6408b8336bac4c4fe3ceb"
          ]
        },
        "id": "w_h_arrjUie2",
        "outputId": "20d7eb51-dfc1-43bf-9bb3-13abacc1ad2f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-0.5B-Instruct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b449b81f0b44802b3a882f6f6723eb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function LLM.__del__ at 0x7aadc5fdc180>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/max/entrypoints/llm.py\", line 89, in __del__\n",
            "    self._pc.set_canceled()\n",
            "    ^^^^^^^^\n",
            "AttributeError: 'LLM' object has no attribute '_pc'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LLM.__init__() takes 2 positional arguments but 3 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4258393756.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpipeline_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m prompts = [\n",
            "\u001b[0;31mTypeError\u001b[0m: LLM.__init__() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    }
  ]
}